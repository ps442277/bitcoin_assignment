{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38b1e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T10:55:57.457699Z",
     "iopub.status.busy": "2022-08-01T10:55:57.457486Z",
     "iopub.status.idle": "2022-08-01T10:55:57.537337Z",
     "shell.execute_reply": "2022-08-01T10:55:57.536373Z",
     "shell.execute_reply.started": "2022-08-01T10:55:57.457669Z"
    }
   },
   "source": [
    "# Adaptive Query Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209532a",
   "metadata": {},
   "source": [
    "Adaptive query execution (AQE) is query re-optimization that occurs during query execution.Here you will be looking at:\n",
    "- Performance with the adaptive query based on:\n",
    "    - Reducing Post Shuffle Partitions\n",
    "    - Dynamically Switching Join Strategies\n",
    "    - Optimizing Skew Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the session\n",
    "print(\"Welcome to my EMR Notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dbb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a file as dataframe to perform analysis\n",
    "spark_df = spark.read.options(inferSchema='True',header='True').csv(\"s3://myawsbucketbuckettt/report.csv\")\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cc727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us now create a tempview to run sql queries on top of the dataframe \n",
    "spark_df.createOrReplaceTempView(\"industry_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ca1fd",
   "metadata": {},
   "source": [
    "# Checking performance when the adaptive query is off vs. on\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8af8b",
   "metadata": {},
   "source": [
    "In order to enable set spark.sql.adaptive.enabled configuration property to 'true'. Let us compare the query performance in Spark with respect to adaptive queries enabled and disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a348133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn adaptive query execution to 'false' to analyse the difference\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running sql query adaptive query execution to 'false'\n",
    "temp_df = spark.sql(\"select * FROM industry_table ORDER BY Year DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffbed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn adaptive query execution to 'true' to analyse the difference\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b1a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running sql query after adaptive query execution to 'true' and show the execution steps/time differnce in terms of partition.\n",
    "temp_df = spark.sql(\"select * FROM industry_table ORDER BY Year DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797f1a4",
   "metadata": {},
   "source": [
    "# Property 1 : Reducing Post Shuffle Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc83cb",
   "metadata": {},
   "source": [
    "Explain the difference how With Spark 3.0; after every stage of the job, Spark dynamically determines the optimal number of partitions by looking at the metrics of the completed stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting adaptive.coalescePartitions to 'false'\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dbd5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a query and getting number of partitions for job when adaptive query and adaptive.coalescePartitions to 'false'\n",
    "temp_df = spark.sql(\"select Year,first(Industry_aggregation_NZSIOC) FROM industry_table group by Year\")\n",
    "print(temp_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59314bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting adaptive.coalescePartitions to 'true'\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a query and getting number of partitions for job when adaptive query and adaptive.coalescePartitions to 'true'. \n",
    "#Compare the number of partitions and explain the DAG difference.\n",
    "\n",
    "temp_df = spark.sql(\"select Year,first(Industry_aggregation_NZSIOC) FROM industry_table group by Year\")\n",
    "print(temp_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1a035",
   "metadata": {},
   "source": [
    "# Property 2 : Dynamically Switching Join Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c249a43",
   "metadata": {},
   "source": [
    "Spark supports several join strategies, and BroadcastHash Join is usually the most performant. With AQE, the size of the composite operation is calculated accurately. And then, Spark now can replan the join strategy. We will be importing two tables and performing a join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing ranking table as dataframe\n",
    "ranking_df = spark.read.options(inferSchema='True',header='True').csv(\"s3://myawsbucketbuckettt/ranking.csv\")\n",
    "ranking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing events table as dataframe\n",
    "events_df = spark.read.options(inferSchema='True',header='True').csv(\"s3://myawsbucketbuckettt/events.csv\")\n",
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us now create a tempview to run sql queries on top of the ranking and events dataframe \n",
    "ranking_df.createOrReplaceTempView(\"ranking_table\")\n",
    "events_df.createOrReplaceTempView(\"events_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enabling the adaptive query\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1702e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing the joins with adaptive query handling broadcast joins\n",
    "joined_df = spark.sql(\"select ranking_table.person_Id, ranking_table.event_Id,events_table.name from ranking_table JOIN events_table ON ranking_table.event_Id = events_table.event_Id\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a85726",
   "metadata": {},
   "source": [
    "In the spark UI, one can see the DAG flow and how to broadcast join handles the smaller dataset to enable efficient joins. Let us now turn off the broadcast join and analyze the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning off broadcast join\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")\n",
    "sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing the joins\n",
    "joined_df = spark.sql(\"select ranking_table.person_Id, ranking_table.event_Id,events_table.name from ranking_table JOIN events_table ON ranking_table.event_Id = events_table.event_Id\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f977d79",
   "metadata": {},
   "source": [
    "# Property 3 : Optimizing Skew Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3083b1a",
   "metadata": {},
   "source": [
    "Skew joins comes into play when the data is unevenly distributed, and one of the partitions is taking a much longer processing time than the others. Let us now analyze the performance of the skewed dataset we have imported above. Some of the event_id in the rankings dataset is more compared to other event_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing the joins alongwith group by statement with skewJoin disabled.\n",
    "opt_joined_df = spark.sql(\"select ranking_table.person_Id, ranking_table.event_Id,events_table.name from ranking_table JOIN events_table ON ranking_table.event_Id = events_table.event_Id\")\n",
    "opt_joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4a5ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#performing the joins alongwith group by statement with skewJoin enabled.\n",
    "opt_joined_df = spark.sql(\"select ranking_table.person_Id, ranking_table.event_Id,events_table.name from ranking_table JOIN events_table ON ranking_table.event_Id = events_table.event_Id\")\n",
    "opt_joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81870fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
